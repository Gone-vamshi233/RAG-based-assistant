This project is part of the Agentic AI Developer Certification (AAIDC) Module 1 and demonstrates the development of a Retrieval-Augmented Generation (RAG) Assistant. The assistant is designed to answer user queries intelligently by combining document retrieval with natural language generation, making AI responses more reliable and context-aware. The main goal of the project is to showcase how retrieval mechanisms can be integrated with language models to produce accurate and fact-based answers. In this system, documents are first loaded from a designated folder, typically in .txt format, allowing the assistant to process both structured and unstructured text. After loading, the documents are split into smaller chunks, ensuring efficient processing and that the language model can handle context within its token limits. Each chunk is then converted into a vector embedding, which is a numerical representation of the text that captures its semantic meaning. This allows the system to identify similarity between user queries and text content, even if the exact wording differs. In this implementation, OpenAI Embeddings are used to generate high-quality vector representations. The embeddings are stored in a vector database, specifically Chroma, which supports fast similarity search and retrieval of the most relevant chunks. When a user submits a question, the assistant retrieves the top relevant chunks from Chroma and passes them to a large language model (LLM), such as GPT-3.5, using the LangChain framework. LangChain orchestrates the workflow from retrieval to generation, creating a seamless pipeline that transforms a user query into an accurate, natural-sounding response. By combining retrieved documents with generative AI capabilities, the assistant can reason over multiple sources and answer complex questions effectively. One key advantage of this RAG architecture is that it reduces hallucinations common in standard generative AI models, as the answers are grounded in actual documents. The system is modular and can be expanded by adding new documents, updating embeddings, or swapping components such as vector databases or embedding models. Tools used in this project include LangChain for workflow orchestration, OpenAI Embeddings for vectorization, Chroma for storage and retrieval, and Streamlit for building a user-friendly interface. This project demonstrates the complete RAG pipeline, from document ingestion to question answering, emphasizing best practices like chunking, embedding, and semantic search. Overall, this project serves as a practical example of how agentic AI can be built to provide context-aware, accurate, and reliable responses, offering both educational insights and a foundation for real-world applications in AI-powered assistants.